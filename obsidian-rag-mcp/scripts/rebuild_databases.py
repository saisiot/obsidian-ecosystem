#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ ì¬ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸

ChromaDBëŠ” ìœ ì§€í•˜ê³ , Networkì™€ Repomix ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë¹Œë“œí•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

import json
from datetime import datetime

from config import METADATA_FILE, VAULT_PATH
from indexer import UnifiedIndexer
from network_store import NetworkMetadataStore
from repomix_store import RepomixIndexStore
from vector_store import VectorStore


def rebuild_databases():
    """ë°ì´í„°ë² ì´ìŠ¤ ì¬ë¹Œë“œ"""

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                                                          â•‘")
    print("â•‘         Obsidian RAG MCP Database Rebuild                â•‘")
    print("â•‘                                                          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # Step 1: ê¸°ì¡´ Networkì™€ Repomix ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
    print("ğŸ“¦ Step 1: ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…")
    print("â”" * 60)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = PROJECT_ROOT / "data" / "backup" / f"rebuild_{timestamp}"
    backup_dir.mkdir(parents=True, exist_ok=True)

    network_file = PROJECT_ROOT / "data" / "network_metadata.json"
    repomix_file = PROJECT_ROOT / "data" / "repomix_index.json"

    if network_file.exists():
        import shutil

        shutil.copy(network_file, backup_dir / "network_metadata.json")
        print(f"  âœ… Network ë°±ì—…: {backup_dir}/network_metadata.json")

    if repomix_file.exists():
        import shutil

        shutil.copy(repomix_file, backup_dir / "repomix_index.json")
        print(f"  âœ… Repomix ë°±ì—…: {backup_dir}/repomix_index.json")

    print()

    # Step 2: Networkì™€ Repomix ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    print("ğŸ”„ Step 2: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”")
    print("â”" * 60)

    # Network Metadata ì´ˆê¸°í™”
    network_store = NetworkMetadataStore(metadata_file=network_file)
    print("  âœ… NetworkMetadataStore ì´ˆê¸°í™”")

    # Repomix Index ì´ˆê¸°í™”
    repomix_store = RepomixIndexStore(index_file=repomix_file)
    print("  âœ… RepomixIndexStore ì´ˆê¸°í™”")

    # VectorStoreëŠ” ê¸°ì¡´ ê²ƒ ì‚¬ìš©
    vector_store = VectorStore()
    print("  âœ… VectorStore ì—°ê²° (ê¸°ì¡´ ChromaDB ì‚¬ìš©)")

    print()

    # Step 3: ì „ì²´ Vault ì¬ì¸ë±ì‹±
    print("ğŸ“‚ Step 3: Vault ì „ì²´ ì¬ì¸ë±ì‹±")
    print("â”" * 60)

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ëª©ë¡
    md_files = list(VAULT_PATH.rglob("*.md"))
    print(f"  ğŸ“‚ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {len(md_files)}ê°œ")
    print()

    # UnifiedIndexer ìƒì„±
    indexer = UnifiedIndexer(
        vector_store=vector_store,
        network_store=network_store,
        repomix_store=repomix_store,
    )

    # index_metadata.jsonì—ì„œ indexed_files ì½ê¸°
    with open(METADATA_FILE, "r", encoding="utf-8") as f:
        metadata = json.load(f)
        indexed_files = metadata.get("indexed_files", {})

    print(f"  ğŸ“Š Index metadata: {len(indexed_files)}ê°œ íŒŒì¼ ê¸°ë¡")
    print("  â³ ì¬ì¸ë±ì‹± ì‹œì‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    # ê° íŒŒì¼ì„ ìˆœíšŒí•˜ë©° Networkì™€ Repomixì— ì¶”ê°€
    from obsidian_parser import ObsidianParser

    parser = ObsidianParser()

    success_count = 0
    error_count = 0

    for i, md_file in enumerate(md_files, 1):
        file_path = str(md_file)

        # indexed_filesì— ìˆëŠ” íŒŒì¼ë§Œ ì²˜ë¦¬
        if file_path not in indexed_files:
            continue

        try:
            # íŒŒì¼ íŒŒì‹±
            doc = parser.parse_file(md_file)
            if not doc:
                continue

            # Network Metadata ì—…ë°ì´íŠ¸
            network_store.update_metadata(doc)

            # Repomix Index ì—…ë°ì´íŠ¸
            repomix_store.update_index(doc, md_file)

            success_count += 1

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 100ê°œë§ˆë‹¤)
            if i % 100 == 0:
                print(f"  â• ì§„í–‰ ì¤‘: {i}/{len(md_files)} ({success_count} ì„±ê³µ)")

        except Exception as e:
            error_count += 1
            if error_count <= 10:  # ì²˜ìŒ 10ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥
                print(f"  âš ï¸  ì—ëŸ¬: {md_file.name} - {e}")

    print()
    print(f"  âœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
    print(f"     - ì„±ê³µ: {success_count}ê°œ")
    print(f"     - ì—ëŸ¬: {error_count}ê°œ")
    print()

    # Networkì™€ Repomix ì €ì¥
    network_store.save_metadata()
    repomix_store.save_index()

    print("  ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    print()

    # Step 4: ê²€ì¦
    print("âœ… Step 4: ì¬ë¹Œë“œ ê²€ì¦")
    print("â”" * 60)

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    assert network_file.exists(), "network_metadata.jsonì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    assert repomix_file.exists(), "repomix_index.jsonì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

    # íŒŒì¼ í¬ê¸° í™•ì¸
    network_size = network_file.stat().st_size
    repomix_size = repomix_file.stat().st_size

    print(f"  âœ… network_metadata.json: {network_size:,} bytes")
    print(f"  âœ… repomix_index.json: {repomix_size:,} bytes")
    print()

    # í†µê³„ ì¶œë ¥
    network_count = len(network_store.metadata["files"])
    repomix_count = len(repomix_store.index["files"])

    print("  ğŸ“Š ì¬ë¹Œë“œ í†µê³„:")
    print(f"     - Network: {network_count}ê°œ íŒŒì¼")
    print(f"     - Repomix: {repomix_count}ê°œ íŒŒì¼")
    print()

    print("â”" * 60)
    print("ğŸ‰ ë°ì´í„°ë² ì´ìŠ¤ ì¬ë¹Œë“œ ì™„ë£Œ!")
    print("â”" * 60)
    print()


if __name__ == "__main__":
    rebuild_databases()
