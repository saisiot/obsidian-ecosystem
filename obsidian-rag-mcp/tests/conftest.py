"""Pytest fixtures for integration tests

í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê³µí†µ fixture ë° helper í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import shutil
import sys
from pathlib import Path

import pytest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from config import VAULT_PATH  # noqa: E402
from indexer import UnifiedIndexer  # noqa: E402
from network_store import NetworkMetadataStore  # noqa: E402
from repomix_store import RepomixIndexStore  # noqa: E402
from vector_store import VectorStore  # noqa: E402


@pytest.fixture(scope="function")
def temp_vault(tmp_path):
    """ì„ì‹œ Vault ë””ë ‰í† ë¦¬ ìƒì„±

    ì‹¤ì œ Vaultë¥¼ ê±´ë“œë¦¬ì§€ ì•Šê³  í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Yields:
        Path: ì„ì‹œ Vault ê²½ë¡œ
    """
    vault_path = tmp_path / "test_vault"
    vault_path.mkdir(parents=True, exist_ok=True)

    # PARA í´ë” êµ¬ì¡° ìƒì„±
    (vault_path / "00 Notes").mkdir(exist_ok=True)
    (vault_path / "01 Reference").mkdir(exist_ok=True)
    (vault_path / "02 Journals").mkdir(exist_ok=True)
    (vault_path / "04 Outputs").mkdir(exist_ok=True)

    yield vault_path

    # Cleanup
    if vault_path.exists():
        shutil.rmtree(vault_path)


@pytest.fixture(scope="function")
def temp_data_dir(tmp_path):
    """ì„ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±

    ChromaDB, ë©”íƒ€ë°ì´í„°, ë°±ì—… íŒŒì¼ì„ ì €ì¥í•  ì„ì‹œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Yields:
        dict: {"root": Path, "chroma": Path, "metadata": Path, "backup": Path}
    """
    data_dir = tmp_path / "test_data"
    data_dir.mkdir(parents=True, exist_ok=True)

    chroma_dir = data_dir / "chroma_db"
    chroma_dir.mkdir(exist_ok=True)

    backup_dir = data_dir / "backup"
    backup_dir.mkdir(exist_ok=True)

    yield {
        "root": data_dir,
        "chroma": chroma_dir,
        "metadata": data_dir / "index_metadata.json",
        "network_metadata": data_dir / "network_metadata.json",
        "repomix_index": data_dir / "repomix_index.json",
        "backup": backup_dir,
    }

    # Cleanup
    if data_dir.exists():
        shutil.rmtree(data_dir)


@pytest.fixture(scope="function")
def sample_notes(temp_vault):
    """ìƒ˜í”Œ ë…¸íŠ¸ íŒŒì¼ ìƒì„±

    ë‹¤ì–‘í•œ ë§í¬, íƒœê·¸, êµ¬ì¡°ë¥¼ ê°€ì§„ í…ŒìŠ¤íŠ¸ìš© ë…¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        temp_vault: ì„ì‹œ Vault ê²½ë¡œ

    Returns:
        list[Path]: ìƒì„±ëœ ë…¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    notes = []

    # Note 1: ê¸°ë³¸ ë…¸íŠ¸ (forward links, tags)
    note1 = temp_vault / "00 Notes" / "Note A.md"
    note1.write_text(
        """---
tags: [python, testing]
---
# Note A

This is a test note that links to [[Note B]] and [[Note C]].

It also has inline tags: #important #draft
"""
    )
    notes.append(note1)

    # Note 2: ë°±ë§í¬ ëŒ€ìƒ
    note2 = temp_vault / "00 Notes" / "Note B.md"
    note2.write_text(
        """---
tags: [research]
---
# Note B

This note is referenced by Note A.

Links to [[Note C]].

#project #active
"""
    )
    notes.append(note2)

    # Note 3: ë‹¤ì¤‘ ë°±ë§í¬ ëŒ€ìƒ
    note3 = temp_vault / "01 Reference" / "Note C.md"
    note3.write_text(
        """# Note C

Referenced by both Note A and Note B.

No forward links, only backlinks.

#reference #core
"""
    )
    notes.append(note3)

    # Note 4: ê³ ë¦½ëœ ë…¸íŠ¸ (no links)
    note4 = temp_vault / "02 Journals" / "Orphaned Note.md"
    note4.write_text(
        """# Orphaned Note

This note has no links at all.

#lonely
"""
    )
    notes.append(note4)

    # Note 5: ê¸´ ì»¨í…ì¸  (ì²­í‚¹ í…ŒìŠ¤íŠ¸)
    note5 = temp_vault / "04 Outputs" / "Long Document.md"
    long_content = "# Long Document\n\n" + "\n\n".join(
        [f"## Section {i}\n\nContent for section {i}. " * 20 for i in range(1, 11)]
    )
    note5.write_text(long_content)
    notes.append(note5)

    return notes


@pytest.fixture(scope="function")
def test_stores(temp_data_dir, monkeypatch):
    """í…ŒìŠ¤íŠ¸ìš© 3ê°œ Store ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    ChromaDB, NetworkMetadataStore, RepomixIndexStoreë¥¼ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        temp_data_dir: ì„ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬
        monkeypatch: pytest monkeypatch fixture

    Yields:
        dict: {"vector": VectorStore, "network": NetworkMetadataStore, "repomix": RepomixIndexStore}
    """
    # config.pyì˜ ê²½ë¡œë¥¼ ì„ì‹œ ë””ë ‰í† ë¦¬ë¡œ íŒ¨ì¹˜
    monkeypatch.setattr("config.CHROMA_PATH", temp_data_dir["chroma"])
    monkeypatch.setattr("config.METADATA_FILE", temp_data_dir["metadata"])
    monkeypatch.setattr("config.BACKUP_DIR", temp_data_dir["backup"])
    monkeypatch.setattr("vector_store.CHROMA_PATH", temp_data_dir["chroma"])

    # ì‹¤ì œ store ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vector_store = VectorStore()

    network_store = NetworkMetadataStore(
        metadata_file=temp_data_dir["network_metadata"]
    )

    repomix_store = RepomixIndexStore(index_file=temp_data_dir["repomix_index"])

    yield {
        "vector": vector_store,
        "network": network_store,
        "repomix": repomix_store,
    }

    # Cleanup: ChromaDB ì»¬ë ‰ì…˜ ì‚­ì œ
    try:
        # ì»¬ë ‰ì…˜ ì™„ì „ ì‚­ì œ
        from config import COLLECTION_NAME

        try:
            vector_store.client.delete_collection(COLLECTION_NAME)
        except Exception:
            pass
    except Exception:
        pass


@pytest.fixture(scope="function")
def unified_indexer(test_stores, temp_vault, temp_data_dir, monkeypatch):
    """UnifiedIndexer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    í…ŒìŠ¤íŠ¸ìš© storesì™€ temp_vaultë¥¼ ì‚¬ìš©í•˜ëŠ” UnifiedIndexerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        test_stores: í…ŒìŠ¤íŠ¸ìš© stores
        temp_vault: ì„ì‹œ Vault ê²½ë¡œ
        temp_data_dir: ì„ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬
        monkeypatch: pytest monkeypatch fixture

    Yields:
        UnifiedIndexer: í…ŒìŠ¤íŠ¸ìš© UnifiedIndexer ì¸ìŠ¤í„´ìŠ¤
    """
    # config.pyì˜ VAULT_PATHë¥¼ ì„ì‹œ Vaultë¡œ íŒ¨ì¹˜
    monkeypatch.setattr("config.VAULT_PATH", temp_vault)
    monkeypatch.setattr("indexer.VAULT_PATH", temp_vault)
    monkeypatch.setattr("obsidian_parser.VAULT_PATH", temp_vault)
    monkeypatch.setattr("repomix_store.VAULT_PATH", temp_vault)

    # indexer.pyì˜ BACKUP_DIRë„ íŒ¨ì¹˜
    monkeypatch.setattr("indexer.BACKUP_DIR", temp_data_dir["backup"])
    monkeypatch.setattr("indexer.METADATA_FILE", temp_data_dir["metadata"])

    # UnifiedIndexer ìƒì„±
    indexer = UnifiedIndexer(
        vector_store=test_stores["vector"],
        network_store=test_stores["network"],
        repomix_store=test_stores["repomix"],
    )

    # vault_path ì˜¤ë²„ë¼ì´ë“œ
    indexer.vault_path = temp_vault

    # metadata ì´ˆê¸°í™” (indexed_files í‚¤ ì¶”ê°€)
    indexer.metadata = {"last_update": 0, "indexed_files": {}}

    yield indexer


@pytest.fixture(scope="function")
def real_vault_files():
    """ì‹¤ì œ Vaultì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¦¬ìŠ¤íŠ¸

    VAULT_PATHì— ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.

    Returns:
        list[Path]: ì‹¤ì œ Vaultì˜ .md íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    """
    if not VAULT_PATH.exists():
        pytest.skip("ì‹¤ì œ Vaultê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    md_files = list(VAULT_PATH.rglob("*.md"))

    if len(md_files) == 0:
        pytest.skip("ì‹¤ì œ Vaultì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

    return md_files


@pytest.fixture(scope="function")
def performance_sample_vault(temp_vault):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ëŒ€ëŸ‰ ìƒ˜í”Œ ë…¸íŠ¸ ìƒì„±

    100ê°œì˜ ë…¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        temp_vault: ì„ì‹œ Vault ê²½ë¡œ

    Returns:
        list[Path]: ìƒì„±ëœ 100ê°œ ë…¸íŠ¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    notes = []

    for i in range(100):
        # í´ë”ë¥¼ ìˆœí™˜í•˜ë©° ë¶„ì‚° ë°°ì¹˜
        folders = ["00 Notes", "01 Reference", "02 Journals", "04 Outputs"]
        folder = folders[i % len(folders)]

        note_path = temp_vault / folder / f"Note_{i:03d}.md"

        # ë‹¤ì–‘í•œ ë§í¬ íŒ¨í„´ ìƒì„±
        links = []
        if i > 0:
            links.append(f"[[Note_{i-1:03d}]]")
        if i < 99:
            links.append(f"[[Note_{i+1:03d}]]")

        content = f"""---
tags: [tag{i % 10}, batch]
---
# Note {i:03d}

This is test note number {i}.

Links: {' '.join(links)}

Content: {"Sample content. " * 10}

#test #performance
"""
        note_path.write_text(content)
        notes.append(note_path)

    return notes


# Helper functions


def count_documents_in_chroma(vector_store: VectorStore) -> int:
    """ChromaDBì˜ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ

    Args:
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤

    Returns:
        int: ì €ì¥ëœ ë¬¸ì„œ(ì²­í¬) ìˆ˜
    """
    result = vector_store.collection.get()
    return len(result["ids"])


def get_unique_files_in_chroma(vector_store: VectorStore) -> set:
    """ChromaDBì— ì €ì¥ëœ ê³ ìœ  íŒŒì¼ ê²½ë¡œ ì§‘í•©

    Args:
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤

    Returns:
        set: ê³ ìœ  íŒŒì¼ ê²½ë¡œ ì§‘í•©
    """
    result = vector_store.collection.get()
    paths = {metadata["path"] for metadata in result["metadatas"]}
    return paths


def verify_three_db_sync(
    vector_store: VectorStore,
    network_store: NetworkMetadataStore,
    repomix_store: RepomixIndexStore,
) -> bool:
    """3ê°œ DBì˜ ë™ê¸°í™” ìƒíƒœ ê²€ì¦

    Args:
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
        network_store: NetworkMetadataStore ì¸ìŠ¤í„´ìŠ¤
        repomix_store: RepomixIndexStore ì¸ìŠ¤í„´ìŠ¤

    Returns:
        bool: ë™ê¸°í™” ìƒíƒœ (True: ë™ê¸°í™”ë¨, False: ë¶ˆì¼ì¹˜)
    """
    chroma_files = get_unique_files_in_chroma(vector_store)
    network_files = set(network_store.metadata["files"].keys())
    repomix_files = set(repomix_store.index["files"].keys())

    return chroma_files == network_files == repomix_files


def print_db_stats(
    vector_store: VectorStore,
    network_store: NetworkMetadataStore,
    repomix_store: RepomixIndexStore,
):
    """3ê°œ DB í†µê³„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)

    Args:
        vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
        network_store: NetworkMetadataStore ì¸ìŠ¤í„´ìŠ¤
        repomix_store: RepomixIndexStore ì¸ìŠ¤í„´ìŠ¤
    """
    chroma_count = count_documents_in_chroma(vector_store)
    chroma_files = len(get_unique_files_in_chroma(vector_store))
    network_count = len(network_store.metadata["files"])
    repomix_count = len(repomix_store.index["files"])

    print("\nğŸ“Š DB í†µê³„:")
    print(f"  - ChromaDB: {chroma_count}ê°œ ì²­í¬, {chroma_files}ê°œ íŒŒì¼")
    print(f"  - NetworkMetadata: {network_count}ê°œ íŒŒì¼")
    print(f"  - RepomixIndex: {repomix_count}ê°œ íŒŒì¼")
