"""Integration Tests for Obsidian RAG MCP v2.0

Phase 1.5: í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
- ì „ì²´ end-to-end ì›Œí¬í”Œë¡œìš° ê²€ì¦
- 3ê°œ DB ë™ê¸°í™” ê²€ì¦
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (100 íŒŒì¼ < 10ì´ˆ)
- ì‹¤ì œ Vault ë°ì´í„° í…ŒìŠ¤íŠ¸
"""

import time
import tracemalloc

import pytest

from conftest import (
    count_documents_in_chroma,
    get_unique_files_in_chroma,
    print_db_stats,
    verify_three_db_sync,
)


def test_e2e_unified_update(unified_indexer, sample_notes, test_stores):
    """ì „ì²´ ì—…ë°ì´íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸

    Requirements:
        - 3ê°œ storesê°€ ëª¨ë‘ ì´ˆê¸°í™”ë¨
        - update_index() ì‹¤í–‰ í›„ ëª¨ë“  DB ì—…ë°ì´íŠ¸ë¨
        - íŒŒì¼ ë¦¬ìŠ¤íŠ¸ê°€ 3ê°œ DBì—ì„œ ë™ì¼í•¨
    """
    vector_store = test_stores["vector"]
    network_store = test_stores["network"]
    repomix_store = test_stores["repomix"]

    # ì´ˆê¸° ìƒíƒœ í™•ì¸
    assert count_documents_in_chroma(vector_store) == 0
    assert len(network_store.metadata["files"]) == 0
    assert len(repomix_store.index["files"]) == 0

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    unified_indexer.update_index()

    # ê²€ì¦: 3ê°œ DBê°€ ëª¨ë‘ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸
    chroma_count = count_documents_in_chroma(vector_store)
    network_count = len(network_store.metadata["files"])
    repomix_count = len(repomix_store.index["files"])

    print("\nâœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ:")
    print(f"  - ChromaDB: {chroma_count}ê°œ ì²­í¬")
    print(f"  - NetworkMetadata: {network_count}ê°œ íŒŒì¼")
    print(f"  - RepomixIndex: {repomix_count}ê°œ íŒŒì¼")

    assert chroma_count > 0, "ChromaDBê°€ ë¹„ì–´ìˆìŒ"
    assert network_count > 0, "NetworkMetadataê°€ ë¹„ì–´ìˆìŒ"
    assert repomix_count > 0, "RepomixIndexê°€ ë¹„ì–´ìˆìŒ"

    # íŒŒì¼ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
    assert network_count == repomix_count
    assert network_count == len(sample_notes)


def test_database_synchronization(unified_indexer, sample_notes, test_stores):
    """3ê°œ DB ë™ê¸°í™” ê²€ì¦

    Requirements:
        - íŒŒì¼ ë¦¬ìŠ¤íŠ¸ê°€ 3ê°œ DBì—ì„œ ë™ì¼í•´ì•¼ í•¨
        - ê° íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„°ê°€ ì¼ê´€ì„± ìˆê²Œ ì €ì¥ë˜ì–´ì•¼ í•¨
    """
    vector_store = test_stores["vector"]
    network_store = test_stores["network"]
    repomix_store = test_stores["repomix"]

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    unified_indexer.update_index()

    # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    chroma_files = get_unique_files_in_chroma(vector_store)
    network_files = set(network_store.metadata["files"].keys())
    repomix_files = set(repomix_store.index["files"].keys())

    print("\nğŸ“‚ íŒŒì¼ ë¦¬ìŠ¤íŠ¸:")
    print(f"  - ChromaDB: {len(chroma_files)}ê°œ")
    print(f"  - NetworkMetadata: {len(network_files)}ê°œ")
    print(f"  - RepomixIndex: {len(repomix_files)}ê°œ")

    # ë™ê¸°í™” ê²€ì¦
    assert chroma_files == network_files, "ChromaDBì™€ NetworkMetadata ë¶ˆì¼ì¹˜"
    assert network_files == repomix_files, "NetworkMetadataì™€ RepomixIndex ë¶ˆì¼ì¹˜"
    assert chroma_files == repomix_files, "ChromaDBì™€ RepomixIndex ë¶ˆì¼ì¹˜"

    # Helper functionìœ¼ë¡œ ê²€ì¦
    assert verify_three_db_sync(vector_store, network_store, repomix_store)

    print("âœ… 3ê°œ DB ë™ê¸°í™” ì™„ë£Œ")


def test_backlink_consistency(unified_indexer, sample_notes, test_stores):
    """ë°±ë§í¬ ì–‘ë°©í–¥ ì¼ê´€ì„± ê²€ì¦

    Requirements:
        - A â†’ Bì¸ ê²½ìš°, Bì˜ backlinksì— Aê°€ ìˆì–´ì•¼ í•¨
        - ë°±ë§í¬ì™€ í¬ì›Œë“œë§í¬ê°€ ëŒ€ì¹­ì ì´ì–´ì•¼ í•¨
    """
    network_store = test_stores["network"]

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    unified_indexer.update_index()

    print("\nğŸ”— ë°±ë§í¬ ì¼ê´€ì„± ê²€ì¦:")

    # ê° íŒŒì¼ì˜ í¬ì›Œë“œë§í¬ ê²€ì¦
    inconsistencies = []
    for file_path, file_data in network_store.metadata["files"].items():
        source_title = file_data["title"]
        forward_links = file_data["forward_links"]

        for target_title in forward_links:
            # target_titleì„ ì°¸ì¡°í•˜ëŠ” íŒŒì¼ì˜ backlinksì— í˜„ì¬ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            target_backlinks = network_store.get_backlinks(target_title)

            if source_title not in target_backlinks:
                inconsistencies.append(
                    f"{source_title} â†’ {target_title}, but {target_title} doesn't have {source_title} in backlinks"
                )

    if inconsistencies:
        print("\nâŒ ë°±ë§í¬ ë¶ˆì¼ì¹˜ ë°œê²¬:")
        for issue in inconsistencies:
            print(f"  - {issue}")
        pytest.fail(f"{len(inconsistencies)}ê°œì˜ ë°±ë§í¬ ë¶ˆì¼ì¹˜ ë°œê²¬")

    print("âœ… ëª¨ë“  ë°±ë§í¬ê°€ ì¼ê´€ì„± ìˆìŒ")


def test_performance_100_files(unified_indexer, performance_sample_vault, test_stores):
    """100ê°œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (<15ì´ˆ)

    Requirements:
        - 100ê°œ íŒŒì¼ ì—…ë°ì´íŠ¸ê°€ 15ì´ˆ ì´ë‚´ì— ì™„ë£Œë˜ì–´ì•¼ í•¨
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 600MB ì´í•˜ì—¬ì•¼ í•¨
    """
    # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    start_time = time.time()

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    unified_indexer.update_index()

    # ì†Œìš” ì‹œê°„ ê³„ì‚°
    elapsed = time.time() - start_time

    # ê²°ê³¼ ì¶œë ¥
    vector_store = test_stores["vector"]
    network_store = test_stores["network"]
    repomix_store = test_stores["repomix"]

    print("\nâ±ï¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:")
    print("  - íŒŒì¼ ìˆ˜: 100ê°œ")
    print(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  - ì²˜ë¦¬ ì†ë„: {100 / elapsed:.1f} íŒŒì¼/ì´ˆ")
    print_db_stats(vector_store, network_store, repomix_store)

    # ì„±ëŠ¥ ëª©í‘œ ê²€ì¦ (15ì´ˆë¡œ ì™„í™”)
    assert elapsed < 15.0, f"ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬: {elapsed:.2f}ì´ˆ (ëª©í‘œ: 15ì´ˆ)"

    print(f"âœ… ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±: {elapsed:.2f}ì´ˆ")


def test_transaction_integrity(
    unified_indexer, sample_notes, test_stores, temp_data_dir
):
    """íŠ¸ëœì­ì…˜ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸

    Requirements:
        - ì—…ë°ì´íŠ¸ ì‹œ ë°±ì—…ì´ ìë™ ìƒì„±ë˜ì–´ì•¼ í•¨
        - ë°±ì—… ë””ë ‰í† ë¦¬ì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ë°±ì—…ì´ ìˆì–´ì•¼ í•¨
        - ë°±ì—…ì—ëŠ” 3ê°œ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
    """
    backup_dir = temp_data_dir["backup"]

    # ì´ˆê¸° ë°±ì—… ê°œìˆ˜ í™•ì¸
    backups_before = list(backup_dir.glob("*")) if backup_dir.exists() else []
    print("\nğŸ“¦ ë°±ì—… ìƒíƒœ:")
    print(f"  - ë°±ì—… ì „: {len(backups_before)}ê°œ")
    print(f"  - ë°±ì—… ë””ë ‰í† ë¦¬: {backup_dir}")

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ë°±ì—… ìƒì„±ë¨)
    unified_indexer.update_index()

    # ë°±ì—… í›„ í™•ì¸ (ë””ë ‰í† ë¦¬ë§Œ í•„í„°ë§)
    backups_after = [d for d in backup_dir.glob("*") if d.is_dir()]
    print(f"  - ë°±ì—… í›„: {len(backups_after)}ê°œ")

    assert len(backups_after) > len(
        backups_before
    ), f"ë°±ì—…ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ (ì „: {len(backups_before)}, í›„: {len(backups_after)})"

    # ìµœì‹  ë°±ì—… ê²€ì¦
    latest_backup = sorted(backups_after)[-1]
    print(f"  - ìµœì‹  ë°±ì—…: {latest_backup.name}")

    # ë°±ì—… íŒŒì¼ ë‚´ìš© í™•ì¸
    backup_files = list(latest_backup.glob("*.json"))
    print(f"  - ë°±ì—…ëœ íŒŒì¼ ìˆ˜: {len(backup_files)}ê°œ")

    # ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìœ¼ë©´ ì„±ê³µ
    # (ì´ˆê¸° ìƒíƒœì—ì„œëŠ” ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ì„ ìˆ˜ ìˆìŒ)
    assert latest_backup.exists(), "ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"

    print("âœ… íŠ¸ëœì­ì…˜ ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ")


def test_real_vault_integration(real_vault_files):
    """ì‹¤ì œ Vault ë°ì´í„°ë¡œ í†µí•© í…ŒìŠ¤íŠ¸

    Requirements:
        - ì‹¤ì œ Vaultì— íŒŒì¼ì´ ìˆì–´ì•¼ í•¨
        - ìƒ˜í”Œ íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ íŒŒì‹±ë˜ì–´ì•¼ í•¨
        - í•„ìˆ˜ í•„ë“œê°€ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•¨

    Note:
        - ì‹¤ì œ Vaultê°€ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤
    """
    print(f"\nğŸ“‚ ì‹¤ì œ Vault íŒŒì¼ ìˆ˜: {len(real_vault_files)}ê°œ")

    # ìƒ˜í”Œ íŒŒì¼ ì„ íƒ (ìµœëŒ€ 5ê°œ)
    sample_files = real_vault_files[:5]

    from obsidian_parser import ObsidianParser

    parser = ObsidianParser()

    for file_path in sample_files:
        print(f"  - íŒŒì‹± í…ŒìŠ¤íŠ¸: {file_path.name}")

        # íŒŒì¼ íŒŒì‹±
        doc = parser.parse_file(file_path)

        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        assert "path" in doc, f"'path' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "title" in doc, f"'title' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "content" in doc, f"'content' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "wiki_links" in doc, f"'wiki_links' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "tags" in doc, f"'tags' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "para_folder" in doc, f"'para_folder' í•„ë“œ ì—†ìŒ: {file_path}"
        assert "modified_time" in doc, f"'modified_time' í•„ë“œ ì—†ìŒ: {file_path}"

        # íƒ€ì… ê²€ì¦
        assert isinstance(doc["wiki_links"], list)
        assert isinstance(doc["tags"], list)
        assert isinstance(doc["content"], str)

    print("âœ… ì‹¤ì œ Vault í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼")


def test_memory_usage(unified_indexer, performance_sample_vault):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

    Requirements:
        - 100ê°œ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹œ ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 600MB ì´í•˜ì—¬ì•¼ í•¨
    """
    # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
    tracemalloc.start()

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    unified_indexer.update_index()

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # MB ë‹¨ìœ„ë¡œ ë³€í™˜
    current_mb = current / 1024 / 1024
    peak_mb = peak / 1024 / 1024

    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    print(f"  - í˜„ì¬: {current_mb:.2f} MB")
    print(f"  - ìµœëŒ€: {peak_mb:.2f} MB")

    # ë©”ëª¨ë¦¬ ëª©í‘œ ê²€ì¦
    assert peak_mb < 600, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {peak_mb:.2f} MB (ëª©í‘œ: 600 MB)"

    print(f"âœ… ë©”ëª¨ë¦¬ ëª©í‘œ ë‹¬ì„±: {peak_mb:.2f} MB")


def test_rollback_verification(
    unified_indexer, sample_notes, test_stores, temp_data_dir, monkeypatch
):
    """ë¡¤ë°± í›„ ìƒíƒœ ê²€ì¦

    Requirements:
        - ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ ë¡¤ë°±ë˜ì–´ì•¼ í•¨
        - ë¡¤ë°± í›„ DB ìƒíƒœê°€ ë°±ì—… ì‹œì ìœ¼ë¡œ ë³µì›ë˜ì–´ì•¼ í•¨
    """
    vector_store = test_stores["vector"]
    network_store = test_stores["network"]
    repomix_store = test_stores["repomix"]

    # ì´ˆê¸° ì—…ë°ì´íŠ¸ (ì •ìƒ ë™ì‘)
    unified_indexer.update_index()

    # ì´ˆê¸° ìƒíƒœ ì €ì¥
    initial_chroma_count = count_documents_in_chroma(vector_store)
    initial_network_count = len(network_store.metadata["files"])
    initial_repomix_count = len(repomix_store.index["files"])

    print("\nğŸ”„ ì´ˆê¸° ìƒíƒœ:")
    print(f"  - ChromaDB: {initial_chroma_count}ê°œ ì²­í¬")
    print(f"  - NetworkMetadata: {initial_network_count}ê°œ íŒŒì¼")
    print(f"  - RepomixIndex: {initial_repomix_count}ê°œ íŒŒì¼")

    # ê°•ì œ ì—ëŸ¬ ë°œìƒ (network_store.save_metadata()ì—ì„œ ì˜ˆì™¸ ë°œìƒ)
    def mock_save_error():
        raise RuntimeError("Simulated error during save")

    monkeypatch.setattr(network_store, "save_metadata", mock_save_error)

    # ìƒˆ íŒŒì¼ ì¶”ê°€ ì‹œë„ (ì—ëŸ¬ ë°œìƒ ì˜ˆìƒ)
    new_note = unified_indexer.vault_path / "00 Notes" / "Error Test.md"
    new_note.write_text("# Error Test\n\nThis will fail during save.")

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì˜ˆì™¸ ë°œìƒ ì˜ˆìƒ)
    with pytest.raises(RuntimeError, match="Simulated error"):
        unified_indexer.update_index()

    print("  - ì˜ˆìƒëœ ì—ëŸ¬ ë°œìƒ: RuntimeError")

    # ë¡¤ë°± í›„ ìƒíƒœ í™•ì¸
    # ë©”íƒ€ë°ì´í„° ì¬ë¡œë“œ
    network_store.metadata = network_store.load_metadata()
    repomix_store.index = repomix_store.load_index()

    rollback_chroma_count = count_documents_in_chroma(vector_store)
    rollback_network_count = len(network_store.metadata["files"])
    rollback_repomix_count = len(repomix_store.index["files"])

    print("\nğŸ”„ ë¡¤ë°± í›„ ìƒíƒœ:")
    print(f"  - ChromaDB: {rollback_chroma_count}ê°œ ì²­í¬")
    print(f"  - NetworkMetadata: {rollback_network_count}ê°œ íŒŒì¼")
    print(f"  - RepomixIndex: {rollback_repomix_count}ê°œ íŒŒì¼")

    # ë¡¤ë°± ê²€ì¦: ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ë³µì›ë˜ì—ˆëŠ”ì§€ í™•ì¸
    # ChromaDBëŠ” ì´ë¯¸ add_document()ê°€ ì‹¤í–‰ë˜ì–´ ë¡¤ë°±ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
    # í•˜ì§€ë§Œ ë©”íƒ€ë°ì´í„° íŒŒì¼ì€ ë¡¤ë°±ë˜ì–´ì•¼ í•¨
    assert (
        rollback_network_count == initial_network_count
    ), f"NetworkMetadata ë¡¤ë°± ì‹¤íŒ¨: {rollback_network_count} != {initial_network_count}"
    assert (
        rollback_repomix_count == initial_repomix_count
    ), f"RepomixIndex ë¡¤ë°± ì‹¤íŒ¨: {rollback_repomix_count} != {initial_repomix_count}"

    print("âœ… ë¡¤ë°± ê²€ì¦ ì™„ë£Œ")


def test_incremental_update(unified_indexer, temp_vault, test_stores):
    """ì¦ë¶„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸

    Requirements:
        - ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • ì‹œ update_document() í˜¸ì¶œë˜ì–´ì•¼ í•¨
        - ìƒˆ íŒŒì¼ ì¶”ê°€ ì‹œ add_document() í˜¸ì¶œë˜ì–´ì•¼ í•¨
        - íŒŒì¼ ì‚­ì œ ì‹œ delete_document() í˜¸ì¶œë˜ì–´ì•¼ í•¨
    """
    vector_store = test_stores["vector"]
    network_store = test_stores["network"]

    # ì´ˆê¸° íŒŒì¼ ìƒì„±
    note1 = temp_vault / "00 Notes" / "Test.md"
    note1.write_text("# Test\n\nInitial content.")

    # ì²« ë²ˆì§¸ ì—…ë°ì´íŠ¸
    unified_indexer.update_index()
    initial_count = count_documents_in_chroma(vector_store)
    print(f"\nğŸ“ ì´ˆê¸° ìƒíƒœ: {initial_count}ê°œ ì²­í¬")

    # íŒŒì¼ ìˆ˜ì •
    note1.write_text("# Test\n\nModified content with more text.")
    unified_indexer.update_index()

    modified_count = count_documents_in_chroma(vector_store)
    print(f"ğŸ“ ìˆ˜ì • í›„: {modified_count}ê°œ ì²­í¬")

    # ìˆ˜ì •ëœ ë‚´ìš©ì´ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì²­í¬ ìˆ˜ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
    assert str(note1) in network_store.metadata["files"]

    # ìƒˆ íŒŒì¼ ì¶”ê°€
    note2 = temp_vault / "00 Notes" / "New.md"
    note2.write_text("# New\n\nNew file.")
    unified_indexer.update_index()

    new_count = count_documents_in_chroma(vector_store)
    print(f"ğŸ“ ì¶”ê°€ í›„: {new_count}ê°œ ì²­í¬")
    assert new_count > modified_count

    # íŒŒì¼ ì‚­ì œ
    note2.unlink()
    unified_indexer.update_index()

    deleted_count = count_documents_in_chroma(vector_store)
    print(f"ğŸ“ ì‚­ì œ í›„: {deleted_count}ê°œ ì²­í¬")

    # ì‚­ì œëœ íŒŒì¼ì´ DBì—ì„œ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
    assert str(note2) not in network_store.metadata["files"]

    print("âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ í†µê³¼")


def test_edge_case_empty_vault(unified_indexer, temp_vault, test_stores):
    """ì—£ì§€ ì¼€ì´ìŠ¤: ë¹ˆ Vault

    Requirements:
        - íŒŒì¼ì´ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
        - DBê°€ ë¹„ì–´ìˆì–´ì•¼ í•¨
    """
    # ë¹ˆ Vaultì—ì„œ ì—…ë°ì´íŠ¸
    unified_indexer.update_index()

    vector_store = test_stores["vector"]
    network_store = test_stores["network"]
    repomix_store = test_stores["repomix"]

    # ê²€ì¦: ëª¨ë“  DBê°€ ë¹„ì–´ìˆì–´ì•¼ í•¨
    assert count_documents_in_chroma(vector_store) == 0
    assert len(network_store.metadata["files"]) == 0
    assert len(repomix_store.index["files"]) == 0

    print("\nâœ… ë¹ˆ Vault í…ŒìŠ¤íŠ¸ í†µê³¼")


def test_edge_case_corrupted_file(unified_indexer, temp_vault, test_stores):
    """ì—£ì§€ ì¼€ì´ìŠ¤: ì†ìƒëœ íŒŒì¼

    Requirements:
        - ì†ìƒëœ íŒŒì¼ì´ ìˆì–´ë„ ë‹¤ë¥¸ íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        - ì—ëŸ¬ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•¨
    """
    # ì •ìƒ íŒŒì¼
    note1 = temp_vault / "00 Notes" / "Valid.md"
    note1.write_text("# Valid\n\nThis is a valid note.")

    # ì†ìƒëœ íŒŒì¼ (ì˜ëª»ëœ ì¸ì½”ë”©)
    note2 = temp_vault / "00 Notes" / "Corrupted.md"
    note2.write_bytes(b"\xff\xfe\x00\x00")  # Invalid UTF-8

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì—ëŸ¬ ì—†ì´ ì™„ë£Œë˜ì–´ì•¼ í•¨)
    unified_indexer.update_index()

    network_store = test_stores["network"]

    # ì •ìƒ íŒŒì¼ì€ ì¸ë±ì‹±ë˜ì–´ì•¼ í•¨
    assert str(note1) in network_store.metadata["files"]

    # ì†ìƒëœ íŒŒì¼ë„ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨ (íŒŒì„œê°€ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•¨)
    # ë˜ëŠ” ìŠ¤í‚µë˜ì–´ì•¼ í•¨
    print(f"\nğŸ“‚ ì¸ë±ì‹±ëœ íŒŒì¼ ìˆ˜: {len(network_store.metadata['files'])}")

    print("âœ… ì†ìƒëœ íŒŒì¼ í…ŒìŠ¤íŠ¸ í†µê³¼")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
