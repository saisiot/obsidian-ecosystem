"""
Context Packer

Obsidian vaultì˜ ë…¸íŠ¸ë“¤ì„ LLMì— ìµœì í™”ëœ í˜•íƒœë¡œ íŒ¨í‚¤ì§•í•˜ëŠ” ì—”ì§„
"""

import sys
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

import tiktoken

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from network_store import NetworkMetadataStore
from obsidian_parser import ObsidianParser
from repomix_store import RepomixIndexStore
from vector_store import VectorStore


class ContextBuilder:
    """ì»¨í…ìŠ¤íŠ¸ ë¹Œë”

    ì£¼ì–´ì§„ ë…¸íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê´€ë ¨ ë…¸íŠ¸ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        vector_store: VectorStore,
        network_store: NetworkMetadataStore,
        repomix_store: RepomixIndexStore,
    ):
        """
        Args:
            vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
            network_store: NetworkMetadataStore ì¸ìŠ¤í„´ìŠ¤
            repomix_store: RepomixIndexStore ì¸ìŠ¤í„´ìŠ¤
        """
        self.vector_store = vector_store
        self.network_store = network_store
        self.repomix_store = repomix_store
        self.parser = ObsidianParser()

    def build_context(
        self,
        note_title: str,
        include_backlinks: bool = True,
        include_forward_links: bool = True,
        include_semantic_related: bool = True,
        include_tag_related: bool = False,
        max_backlinks: int = 10,
        max_forward_links: int = 10,
        max_semantic_related: int = 5,
        max_tag_related: int = 5,
    ) -> Dict[str, List[dict]]:
        """ì£¼ì–´ì§„ ë…¸íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±

        Args:
            note_title: ì¤‘ì‹¬ ë…¸íŠ¸ ì œëª©
            include_backlinks: ë°±ë§í¬ í¬í•¨ ì—¬ë¶€
            include_forward_links: í¬ì›Œë“œë§í¬ í¬í•¨ ì—¬ë¶€
            include_semantic_related: ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ í¬í•¨ ì—¬ë¶€
            include_tag_related: íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ í¬í•¨ ì—¬ë¶€
            max_backlinks: ìµœëŒ€ ë°±ë§í¬ ìˆ˜
            max_forward_links: ìµœëŒ€ í¬ì›Œë“œë§í¬ ìˆ˜
            max_semantic_related: ìµœëŒ€ ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ ìˆ˜
            max_tag_related: ìµœëŒ€ íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ ìˆ˜

        Returns:
            ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
            {
                "primary": [ì£¼ ë…¸íŠ¸],
                "backlinks": [ë°±ë§í¬ ë…¸íŠ¸ë“¤],
                "forward_links": [í¬ì›Œë“œë§í¬ ë…¸íŠ¸ë“¤],
                "semantic_related": [ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ë“¤],
                "tag_related": [íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ë“¤]
            }
        """
        context = {
            "primary": [],
            "backlinks": [],
            "forward_links": [],
            "semantic_related": [],
            "tag_related": [],
        }

        # 1. ì£¼ ë…¸íŠ¸ ì°¾ê¸°
        primary_note = self._get_note_by_title(note_title)
        if not primary_note:
            return context

        context["primary"] = [primary_note]

        # ì²˜ë¦¬ëœ ë…¸íŠ¸ ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        processed_paths: Set[str] = {primary_note["path"]}

        # 2. ë°±ë§í¬ ìˆ˜ì§‘
        if include_backlinks:
            backlink_titles = self.network_store.get_backlinks(note_title)
            for title in backlink_titles[:max_backlinks]:
                note = self._get_note_by_title(title)
                if note and note["path"] not in processed_paths:
                    context["backlinks"].append(note)
                    processed_paths.add(note["path"])

        # 3. í¬ì›Œë“œë§í¬ ìˆ˜ì§‘
        if include_forward_links:
            forward_link_titles = self.network_store.get_forward_links(note_title)
            for title in forward_link_titles[:max_forward_links]:
                note = self._get_note_by_title(title)
                if note and note["path"] not in processed_paths:
                    context["forward_links"].append(note)
                    processed_paths.add(note["path"])

        # 4. ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ ìˆ˜ì§‘
        if include_semantic_related:
            # ì£¼ ë…¸íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì‚¬ ê²€ìƒ‰
            results = self.vector_store.search(
                query=primary_note["content"][:500],  # ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                top_k=max_semantic_related + 1,  # ìê¸° ìì‹  ì œì™¸
            )

            for result in results:
                if result["path"] not in processed_paths:
                    note = self._get_note_by_path(result["path"])
                    if note:
                        note["similarity_score"] = result.get("distance", 0.0)
                        context["semantic_related"].append(note)
                        processed_paths.add(note["path"])

                if len(context["semantic_related"]) >= max_semantic_related:
                    break

        # 5. íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ ìˆ˜ì§‘
        if include_tag_related and primary_note.get("tags"):
            # ì£¼ ë…¸íŠ¸ì˜ ì²« ë²ˆì§¸ íƒœê·¸ ì‚¬ìš©
            main_tag = primary_note["tags"][0]
            tag_note_titles = self.network_store.get_notes_by_tag(main_tag)

            for title in tag_note_titles[:max_tag_related]:
                note = self._get_note_by_title(title)
                if note and note["path"] not in processed_paths:
                    context["tag_related"].append(note)
                    processed_paths.add(note["path"])

        return context

    def _get_note_by_title(self, title: str) -> Optional[dict]:
        """ë…¸íŠ¸ ì œëª©ìœ¼ë¡œ ë…¸íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

        Args:
            title: ë…¸íŠ¸ ì œëª©

        Returns:
            ë…¸íŠ¸ ì •ë³´ (path, title, content, tags ë“±)
        """
        # network_storeì—ì„œ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        file_path = self.network_store._find_file_by_title(title)
        if not file_path:
            return None

        return self._get_note_by_path(file_path)

    def _get_note_by_path(self, path: str) -> Optional[dict]:
        """íŒŒì¼ ê²½ë¡œë¡œ ë…¸íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°

        Args:
            path: íŒŒì¼ ê²½ë¡œ

        Returns:
            ë…¸íŠ¸ ì •ë³´
        """
        try:
            file_path = Path(path)
            if not file_path.exists():
                return None

            # íŒŒì¼ íŒŒì‹±
            doc = self.parser.parse_file(file_path)
            if not doc:
                return None

            # repomix_storeì—ì„œ í† í° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            repomix_data = self.repomix_store.index.get("files", {}).get(path, {})
            if repomix_data:
                doc["token_count"] = repomix_data.get("size", {}).get(
                    "estimated_tokens", 0
                )
            else:
                doc["token_count"] = 0

            return doc

        except Exception as e:
            print(f"âš ï¸ ë…¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({path}): {e}")
            return None


class SmartPacker:
    """ìŠ¤ë§ˆíŠ¸ íŒ¨ì»¤

    í† í° ì œí•œ ë‚´ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœì ìœ¼ë¡œ íŒ¨í‚¹í•©ë‹ˆë‹¤.
    """

    def __init__(self, max_tokens: int = 100000, tokenizer_encoding: str = "cl100k_base"):
        """
        Args:
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            tokenizer_encoding: í† í¬ë‚˜ì´ì € ì¸ì½”ë”©
        """
        self.max_tokens = max_tokens
        self.tokenizer = tiktoken.get_encoding(tokenizer_encoding)

    def pack(
        self,
        context: Dict[str, List[dict]],
        priorities: Optional[Dict[str, float]] = None,
    ) -> Dict[str, List[dict]]:
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œ ë‚´ì—ì„œ íŒ¨í‚¹

        Args:
            context: ContextBuilder.build_context() ê²°ê³¼
            priorities: ê° ì„¹ì…˜ì˜ ìš°ì„ ìˆœìœ„ (ê¸°ë³¸ê°’: primary=1.0, backlinks=0.8, ...)

        Returns:
            íŒ¨í‚¹ëœ ì»¨í…ìŠ¤íŠ¸ (ì›ë³¸ êµ¬ì¡° ìœ ì§€, ì¼ë¶€ ë…¸íŠ¸ëŠ” íŠ¸ë¦¬ë°ë˜ê±°ë‚˜ ì œì™¸ë¨)
        """
        if priorities is None:
            priorities = {
                "primary": 1.0,
                "backlinks": 0.8,
                "forward_links": 0.7,
                "semantic_related": 0.6,
                "tag_related": 0.5,
            }

        # í† í° ì˜ˆì‚° ì´ˆê¸°í™”
        remaining_tokens = self.max_tokens
        packed_context = {
            "primary": [],
            "backlinks": [],
            "forward_links": [],
            "semantic_related": [],
            "tag_related": [],
        }

        # ì„¹ì…˜ë³„ ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ì²˜ë¦¬
        section_order = sorted(priorities.keys(), key=lambda k: priorities[k], reverse=True)

        for section in section_order:
            notes = context.get(section, [])
            if not notes:
                continue

            for note in notes:
                token_count = note.get("token_count", 0)

                # í† í° ì¹´ìš´íŠ¸ê°€ 0ì´ë©´ ì§ì ‘ ê³„ì‚°
                if token_count == 0:
                    try:
                        content = note.get("content", "")
                        token_count = len(self.tokenizer.encode(content))
                    except:
                        token_count = len(note.get("content", "")) // 4

                # í† í°ì´ ë‚¨ì•„ìˆìœ¼ë©´ ì¶”ê°€
                if token_count <= remaining_tokens:
                    packed_context[section].append(note)
                    remaining_tokens -= token_count
                else:
                    # í† í°ì´ ë¶€ì¡±í•˜ë©´ ì»¨í…ì¸  íŠ¸ë¦¬ë° ì‹œë„
                    if remaining_tokens > 100:  # ìµœì†Œ 100 í† í° ì´ìƒ ë‚¨ì•˜ì„ ë•Œë§Œ
                        trimmed_note = self._trim_note(note, remaining_tokens)
                        if trimmed_note:
                            packed_context[section].append(trimmed_note)
                            remaining_tokens -= trimmed_note["token_count"]

        return packed_context

    def _trim_note(self, note: dict, max_tokens: int) -> Optional[dict]:
        """ë…¸íŠ¸ ì»¨í…ì¸ ë¥¼ í† í° ì œí•œì— ë§ê²Œ íŠ¸ë¦¬ë°

        Args:
            note: ë…¸íŠ¸ ì •ë³´
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            íŠ¸ë¦¬ë°ëœ ë…¸íŠ¸ (Noneì´ë©´ íŠ¸ë¦¬ë° ë¶ˆê°€)
        """
        content = note.get("content", "")
        if not content:
            return None

        # í† í° ì œí•œì— ë§ê²Œ ë¬¸ì ì¶”ì • (1 í† í° â‰ˆ 4 ë¬¸ì)
        max_chars = max_tokens * 4

        if len(content) <= max_chars:
            return note

        # ì»¨í…ì¸  íŠ¸ë¦¬ë°
        trimmed_content = content[:max_chars] + "\n\n... (ë‚´ìš©ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤)"

        # ìƒˆ ë…¸íŠ¸ ê°ì²´ ìƒì„±
        trimmed_note = note.copy()
        trimmed_note["content"] = trimmed_content
        trimmed_note["token_count"] = max_tokens
        trimmed_note["trimmed"] = True

        return trimmed_note


class PackageFormatter:
    """íŒ¨í‚¤ì§€ í¬ë§¤í„°

    íŒ¨í‚¹ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì— ìµœì í™”ëœ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
    """

    def format(
        self,
        packed_context: Dict[str, List[dict]],
        include_metadata: bool = True,
        include_links: bool = True,
    ) -> str:
        """íŒ¨í‚¹ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…

        Args:
            packed_context: SmartPacker.pack() ê²°ê³¼
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            include_links: ë§í¬ ì •ë³´ í¬í•¨ ì—¬ë¶€

        Returns:
            í¬ë§·íŒ…ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        """
        output = []

        # í—¤ë”
        output.append("# ğŸ“¦ Obsidian Context Package\n")

        # í†µê³„
        total_notes = sum(len(notes) for notes in packed_context.values())
        output.append(f"**Total Notes**: {total_notes}\n")
        output.append("---\n")

        # ì„¹ì…˜ë³„ ì¶œë ¥
        section_titles = {
            "primary": "ğŸ¯ Primary Note",
            "backlinks": "â¬…ï¸ Backlinks",
            "forward_links": "â¡ï¸ Forward Links",
            "semantic_related": "ğŸ”— Semantically Related",
            "tag_related": "ğŸ·ï¸ Tag Related",
        }

        for section, title in section_titles.items():
            notes = packed_context.get(section, [])
            if not notes:
                continue

            output.append(f"\n## {title} ({len(notes)} notes)\n")

            for i, note in enumerate(notes, 1):
                output.append(f"\n### {i}. {note['title']}\n")

                # ë©”íƒ€ë°ì´í„°
                if include_metadata:
                    output.append(f"**Path**: `{note['path']}`  \n")
                    output.append(f"**Folder**: {note.get('para_folder', 'N/A')}  \n")

                    if note.get("tags"):
                        tags_str = ", ".join(f"#{tag}" for tag in note["tags"])
                        output.append(f"**Tags**: {tags_str}  \n")

                    if note.get("trimmed"):
                        output.append("âš ï¸ **Note**: Content trimmed due to token limit  \n")

                # ë§í¬ ì •ë³´
                if include_links and note.get("wiki_links"):
                    links_str = ", ".join(f"[[{link}]]" for link in note["wiki_links"][:5])
                    output.append(f"**Links**: {links_str}  \n")

                # ì»¨í…ì¸ 
                output.append("\n---\n")
                output.append(f"{note['content']}\n")
                output.append("---\n")

        return "".join(output)


class ContextPacker:
    """í†µí•© Context Packer

    ContextBuilder, SmartPacker, PackageFormatterë¥¼ í†µí•©í•œ í¸ì˜ í´ë˜ìŠ¤
    """

    def __init__(
        self,
        vector_store: VectorStore,
        network_store: NetworkMetadataStore,
        repomix_store: RepomixIndexStore,
        max_tokens: int = 100000,
    ):
        """
        Args:
            vector_store: VectorStore ì¸ìŠ¤í„´ìŠ¤
            network_store: NetworkMetadataStore ì¸ìŠ¤í„´ìŠ¤
            repomix_store: RepomixIndexStore ì¸ìŠ¤í„´ìŠ¤
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        """
        self.context_builder = ContextBuilder(vector_store, network_store, repomix_store)
        self.smart_packer = SmartPacker(max_tokens=max_tokens)
        self.formatter = PackageFormatter()

    def pack_note(
        self,
        note_title: str,
        include_backlinks: bool = True,
        include_forward_links: bool = True,
        include_semantic_related: bool = True,
        include_tag_related: bool = False,
        max_backlinks: int = 10,
        max_forward_links: int = 10,
        max_semantic_related: int = 5,
        max_tag_related: int = 5,
        include_metadata: bool = True,
        include_links: bool = True,
    ) -> str:
        """ë…¸íŠ¸ì™€ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ íŒ¨í‚¤ì§•

        Args:
            note_title: ì¤‘ì‹¬ ë…¸íŠ¸ ì œëª©
            include_backlinks: ë°±ë§í¬ í¬í•¨ ì—¬ë¶€
            include_forward_links: í¬ì›Œë“œë§í¬ í¬í•¨ ì—¬ë¶€
            include_semantic_related: ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ í¬í•¨ ì—¬ë¶€
            include_tag_related: íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ í¬í•¨ ì—¬ë¶€
            max_backlinks: ìµœëŒ€ ë°±ë§í¬ ìˆ˜
            max_forward_links: ìµœëŒ€ í¬ì›Œë“œë§í¬ ìˆ˜
            max_semantic_related: ìµœëŒ€ ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ ìˆ˜
            max_tag_related: ìµœëŒ€ íƒœê·¸ ê´€ë ¨ ë…¸íŠ¸ ìˆ˜
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€
            include_links: ë§í¬ ì •ë³´ í¬í•¨ ì—¬ë¶€

        Returns:
            í¬ë§·íŒ…ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        """
        # 1. ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        context = self.context_builder.build_context(
            note_title=note_title,
            include_backlinks=include_backlinks,
            include_forward_links=include_forward_links,
            include_semantic_related=include_semantic_related,
            include_tag_related=include_tag_related,
            max_backlinks=max_backlinks,
            max_forward_links=max_forward_links,
            max_semantic_related=max_semantic_related,
            max_tag_related=max_tag_related,
        )

        # 2. ìŠ¤ë§ˆíŠ¸ íŒ¨í‚¹
        packed_context = self.smart_packer.pack(context)

        # 3. í¬ë§·íŒ…
        formatted_output = self.formatter.format(
            packed_context,
            include_metadata=include_metadata,
            include_links=include_links,
        )

        return formatted_output
