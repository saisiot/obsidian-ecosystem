import asyncio
import sys
from pathlib import Path
from typing import Optional, List
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from config import *
from vector_store import VectorStore
from indexer import UnifiedIndexer
from network_store import NetworkMetadataStore
from repomix_store import RepomixIndexStore
from obsidian_parser import ObsidianParser
from auto_update_service import AutoUpdateService
from context_packer import ContextPacker

# ì„œë²„ ì´ˆê¸°í™”
server = Server("obsidian-rag")
vector_store = None
indexer = None
parser = ObsidianParser()
auto_update_service = None
context_packer = None

@server.list_tools()
async def handle_list_tools() -> list[types.Tool]:
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
    return [
        types.Tool(
            name="search_notes",
            description="Obsidian ë…¸íŠ¸ë¥¼ ì‹œë§¨í‹± ê²€ìƒ‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "ê²€ìƒ‰ ì¿¼ë¦¬"},
                    "top_k": {"type": "integer", "description": "ê²°ê³¼ ê°œìˆ˜", "default": 5},
                    "folder": {"type": "string", "description": "PARA í´ë” í•„í„°"}
                },
                "required": ["query"]
            }
        ),
        types.Tool(
            name="get_note",
            description="íŠ¹ì • ë…¸íŠ¸ì˜ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "ë…¸íŠ¸ ì œëª©"}
                },
                "required": ["title"]
            }
        ),
        types.Tool(
            name="find_related",
            description="ì—°ê´€ëœ ë…¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "note_path": {"type": "string", "description": "ë…¸íŠ¸ ê²½ë¡œ"},
                    "top_k": {"type": "integer", "description": "ê²°ê³¼ ê°œìˆ˜", "default": 5}
                },
                "required": ["note_path"]
            }
        ),
        types.Tool(
            name="search_by_tag",
            description="íƒœê·¸ë¡œ ë…¸íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "tag": {"type": "string", "description": "ê²€ìƒ‰í•  íƒœê·¸"}
                },
                "required": ["tag"]
            }
        ),
        types.Tool(
            name="get_backlinks",
            description="íŠ¹ì • ë…¸íŠ¸ë¥¼ ì°¸ì¡°í•˜ëŠ” ëª¨ë“  ë…¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {
                    "note_title": {"type": "string", "description": "ë…¸íŠ¸ ì œëª©"}
                },
                "required": ["note_title"]
            }
        ),
        types.Tool(
            name="get_vault_stats",
            description="ë³¼íŠ¸ í†µê³„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="update_index",
            description="ì¸ë±ìŠ¤ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤ (ìë™ ì—…ë°ì´íŠ¸ ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ, í•„ìš”ì‹œ ì¦‰ì‹œ ìˆ˜ë™ ì—…ë°ì´íŠ¸ ê°€ëŠ¥)",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        types.Tool(
            name="pack_note_context",
            description="ë…¸íŠ¸ì™€ ê´€ë ¨ëœ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì— ìµœì í™”ëœ í˜•íƒœë¡œ íŒ¨í‚¤ì§•í•©ë‹ˆë‹¤ (ë°±ë§í¬, í¬ì›Œë“œë§í¬, ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ í¬í•¨)",
            inputSchema={
                "type": "object",
                "properties": {
                    "note_title": {"type": "string", "description": "íŒ¨í‚¤ì§•í•  ë…¸íŠ¸ ì œëª©"},
                    "max_tokens": {"type": "integer", "description": "ìµœëŒ€ í† í° ìˆ˜", "default": 100000},
                    "include_backlinks": {"type": "boolean", "description": "ë°±ë§í¬ í¬í•¨", "default": True},
                    "include_forward_links": {"type": "boolean", "description": "í¬ì›Œë“œë§í¬ í¬í•¨", "default": True},
                    "include_semantic_related": {"type": "boolean", "description": "ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ í¬í•¨", "default": True},
                    "max_backlinks": {"type": "integer", "description": "ìµœëŒ€ ë°±ë§í¬ ìˆ˜", "default": 10},
                    "max_forward_links": {"type": "integer", "description": "ìµœëŒ€ í¬ì›Œë“œë§í¬ ìˆ˜", "default": 10},
                    "max_semantic_related": {"type": "integer", "description": "ìµœëŒ€ ì‹œë§¨í‹± ìœ ì‚¬ ë…¸íŠ¸ ìˆ˜", "default": 5}
                },
                "required": ["note_title"]
            }
        )
    ]

@server.call_tool()
async def handle_call_tool(
    name: str,
    arguments: dict
) -> list[types.TextContent]:
    """ë„êµ¬ ì‹¤í–‰"""

    if name == "search_notes":
        results = vector_store.search(
            query=arguments["query"],
            top_k=arguments.get("top_k", 5),
            folder=arguments.get("folder")
        )

        response = f"ğŸ” '{arguments['query']}' ê²€ìƒ‰ ê²°ê³¼:\n\n"
        for i, result in enumerate(results, 1):
            response += f"{i}. **{result['title']}**\n"
            response += f"   ğŸ“ {result['metadata']['para_folder']}\n"
            response += f"   ğŸ“ {result['content'][:200]}...\n"
            response += f"   ğŸ·ï¸ {result['metadata'].get('tags', 'ì—†ìŒ')}\n\n"

        return [types.TextContent(type="text", text=response)]

    elif name == "get_note":
        # ë…¸íŠ¸ ì°¾ê¸° ë¡œì§
        title = arguments["title"]
        results = vector_store.collection.get(
            where={"title": title}
        )

        if results['ids']:
            # ë™ì¼í•œ ê²½ë¡œì˜ ëª¨ë“  ì²­í¬ë¥¼ ëª¨ì•„ì„œ ì „ì²´ ë‚´ìš© ì¬êµ¬ì„±
            path = results['metadatas'][0]['path']
            all_chunks = vector_store.collection.get(
                where={"path": path}
            )

            # chunk_index ìˆœì„œë¡œ ì •ë ¬
            chunks_with_index = list(zip(
                all_chunks['metadatas'],
                all_chunks['documents']
            ))
            chunks_with_index.sort(key=lambda x: x[0]['chunk_index'])

            full_content = '\n'.join([chunk[1] for chunk in chunks_with_index])
            metadata = chunks_with_index[0][0]

            response = f"ğŸ“„ **{title}**\n\n"
            response += f"ğŸ“ í´ë”: {metadata['para_folder']}\n"
            response += f"ğŸ·ï¸ íƒœê·¸: {metadata.get('tags', 'ì—†ìŒ')}\n"
            response += f"ğŸ”— ìœ„í‚¤ë§í¬: {metadata.get('wiki_links', 'ì—†ìŒ')}\n\n"
            response += f"**ë‚´ìš©:**\n{full_content}"

            return [types.TextContent(type="text", text=response)]
        else:
            return [types.TextContent(type="text", text=f"'{title}' ë…¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]

    elif name == "find_related":
        # ì—°ê´€ ë…¸íŠ¸ ì°¾ê¸°
        note_path = arguments["note_path"]
        # í˜„ì¬ ë…¸íŠ¸ ì½ê¸°
        note_file = Path(note_path)
        if note_file.exists():
            doc = parser.parse_file(note_file)
            # ë…¸íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì‚¬ ê²€ìƒ‰
            results = vector_store.search(
                query=doc['content'][:500],  # ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                top_k=arguments.get("top_k", 5) + 1  # ìê¸° ìì‹  ì œì™¸
            )

            # ìê¸° ìì‹  ì œì™¸
            results = [r for r in results if r['path'] != note_path][:arguments.get("top_k", 5)]

            response = f"ğŸ”— '{doc['title']}'ì™€ ì—°ê´€ëœ ë…¸íŠ¸:\n\n"
            for i, result in enumerate(results, 1):
                response += f"{i}. **{result['title']}**\n"
                response += f"   ğŸ“ {result['metadata']['para_folder']}\n"
                response += f"   ğŸ“ {result['content'][:200]}...\n\n"

            return [types.TextContent(type="text", text=response)]
        else:
            return [types.TextContent(type="text", text=f"'{note_path}' ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]

    elif name == "search_by_tag":
        # íƒœê·¸ ê²€ìƒ‰
        tag = arguments["tag"]
        results = vector_store.collection.get(
            where_document={"$contains": f"#{tag}"}
        )

        # ì¤‘ë³µ ì œê±° (path ê¸°ì¤€)
        unique_notes = {}
        for i, metadata in enumerate(results['metadatas']):
            path = metadata['path']
            if path not in unique_notes:
                unique_notes[path] = {
                    'title': metadata['title'],
                    'para_folder': metadata['para_folder'],
                    'tags': metadata.get('tags', '')
                }

        response = f"ğŸ·ï¸ '#{tag}' íƒœê·¸ê°€ ìˆëŠ” ë…¸íŠ¸ ({len(unique_notes)}ê°œ):\n\n"
        for i, (path, note) in enumerate(unique_notes.items(), 1):
            response += f"{i}. **{note['title']}**\n"
            response += f"   ğŸ“ {note['para_folder']}\n"
            response += f"   ğŸ·ï¸ {note['tags']}\n\n"

        return [types.TextContent(type="text", text=response)]

    elif name == "get_backlinks":
        # ë°±ë§í¬ ì°¾ê¸°
        note_title = arguments["note_title"]

        # ëª¨ë“  ë…¸íŠ¸ì—ì„œ ìœ„í‚¤ë§í¬ ê²€ìƒ‰
        results = vector_store.collection.get()

        backlinks = set()
        for metadata in results['metadatas']:
            wiki_links = metadata.get('wiki_links', '').split(',')
            if note_title in wiki_links:
                backlinks.add((metadata['path'], metadata['title'], metadata['para_folder']))

        response = f"â¬…ï¸ '{note_title}'ë¥¼ ì°¸ì¡°í•˜ëŠ” ë…¸íŠ¸ ({len(backlinks)}ê°œ):\n\n"
        for i, (path, title, folder) in enumerate(sorted(backlinks), 1):
            response += f"{i}. **{title}**\n"
            response += f"   ğŸ“ {folder}\n"
            response += f"   ğŸ“„ {path}\n\n"

        return [types.TextContent(type="text", text=response)]

    elif name == "get_vault_stats":
        # í†µê³„ ìƒì„±
        total_notes = len(indexer.metadata['indexed_files'])

        # PARA í´ë”ë³„ ë¶„í¬
        para_distribution = {}
        for file_path in indexer.metadata['indexed_files'].keys():
            path = Path(file_path)
            try:
                para_folder = path.relative_to(VAULT_PATH).parts[0]
                para_distribution[para_folder] = para_distribution.get(para_folder, 0) + 1
            except:
                pass

        response = "ğŸ“Š **Vault í†µê³„**\n\n"
        response += f"ğŸ“ ì „ì²´ ë…¸íŠ¸ ìˆ˜: {total_notes}ê°œ\n"
        response += f"â° ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {indexer.metadata.get('last_update', 'Never')}\n\n"
        response += "**PARA í´ë”ë³„ ë¶„í¬:**\n"
        for folder, count in sorted(para_distribution.items()):
            response += f"  - {folder}: {count}ê°œ\n"

        return [types.TextContent(type="text", text=response)]

    elif name == "update_index":
        # ì¸ë±ìŠ¤ ìˆ˜ë™ ì—…ë°ì´íŠ¸
        print("ğŸ“Š ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì‹œì‘...", file=sys.stderr)
        updates = indexer.check_updates()

        response = "ğŸ”„ **ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸**\n\n"
        response += f"ğŸ“¥ ìƒˆ íŒŒì¼: {len(updates['new'])}ê°œ\n"
        response += f"ğŸ“ ìˆ˜ì •ëœ íŒŒì¼: {len(updates['modified'])}ê°œ\n"
        response += f"ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼: {len(updates['deleted'])}ê°œ\n\n"

        if any(updates.values()):
            indexer.update_index()
            response += "âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!"
        else:
            response += "âœ… ë³€ê²½ì‚¬í•­ ì—†ìŒ. ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤."

        print("âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ", file=sys.stderr)
        return [types.TextContent(type="text", text=response)]

    elif name == "pack_note_context":
        # ë…¸íŠ¸ ì»¨í…ìŠ¤íŠ¸ íŒ¨í‚¤ì§•
        note_title = arguments["note_title"]
        max_tokens = arguments.get("max_tokens", 100000)

        print(f"ğŸ“¦ '{note_title}' ì»¨í…ìŠ¤íŠ¸ íŒ¨í‚¤ì§• ì‹œì‘...", file=sys.stderr)

        try:
            # ContextPackerì— max_tokens ì„¤ì • ì ìš©
            context_packer.smart_packer.max_tokens = max_tokens

            # ë…¸íŠ¸ íŒ¨í‚¤ì§•
            packed_content = context_packer.pack_note(
                note_title=note_title,
                include_backlinks=arguments.get("include_backlinks", True),
                include_forward_links=arguments.get("include_forward_links", True),
                include_semantic_related=arguments.get("include_semantic_related", True),
                include_tag_related=False,
                max_backlinks=arguments.get("max_backlinks", 10),
                max_forward_links=arguments.get("max_forward_links", 10),
                max_semantic_related=arguments.get("max_semantic_related", 5),
            )

            print("âœ… ì»¨í…ìŠ¤íŠ¸ íŒ¨í‚¤ì§• ì™„ë£Œ", file=sys.stderr)
            return [types.TextContent(type="text", text=packed_content)]

        except Exception as e:
            error_msg = f"âŒ íŒ¨í‚¤ì§• ì‹¤íŒ¨: {str(e)}"
            print(error_msg, file=sys.stderr)
            return [types.TextContent(type="text", text=error_msg)]

    return [types.TextContent(type="text", text="ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ")]

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    global vector_store, indexer, auto_update_service, context_packer

    print("ğŸš€ Obsidian RAG MCP ì„œë²„ ì‹œì‘...", file=sys.stderr)
    print(f"ğŸ“ Vault ê²½ë¡œ: {VAULT_PATH}", file=sys.stderr)

    # 3ê°œ Store ì´ˆê¸°í™”
    print("ğŸ”§ Store ì´ˆê¸°í™” ì¤‘...", file=sys.stderr)
    vector_store = VectorStore()
    network_store = NetworkMetadataStore()
    repomix_store = RepomixIndexStore()

    # UnifiedIndexer ì´ˆê¸°í™” (3ê°œ DB í†µí•© ê´€ë¦¬)
    indexer = UnifiedIndexer(vector_store, network_store, repomix_store)

    # ì¸ë±ìŠ¤ê°€ ì—†ì„ ë•Œë§Œ ì´ˆê¸° ì¸ë±ì‹±
    if not indexer.metadata.get('indexed_files'):
        print("ğŸ“Š ì´ˆê¸° ì¸ë±ì‹± ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œì—ë§Œ)", file=sys.stderr)
        indexer.update_index()
    else:
        print(f"âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(indexer.metadata['indexed_files'])}ê°œ íŒŒì¼)", file=sys.stderr)

    # ContextPacker ì´ˆê¸°í™”
    print("ğŸ“¦ ContextPacker ì´ˆê¸°í™” ì¤‘...", file=sys.stderr)
    context_packer = ContextPacker(vector_store, network_store, repomix_store, max_tokens=100000)

    # Auto-Update Service ì‹œì‘
    print("ğŸ”„ Auto-Update Service ì‹œì‘ ì¤‘...", file=sys.stderr)
    auto_update_service = AutoUpdateService(indexer, debounce_seconds=5.0)
    auto_update_service.start()

    print("ğŸ‰ MCP ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!", file=sys.stderr)

    try:
        # MCP ì„œë²„ ì‹œì‘
        async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="obsidian-rag",
                    server_version="2.0.0",
                    capabilities=server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={}
                    )
                )
            )
    finally:
        # ì„œë²„ ì¢…ë£Œ ì‹œ Auto-Update Serviceë„ ì¤‘ì§€
        if auto_update_service:
            print("â¹ï¸ Auto-Update Service ì¤‘ì§€ ì¤‘...", file=sys.stderr)
            auto_update_service.stop()

if __name__ == "__main__":
    asyncio.run(main())
