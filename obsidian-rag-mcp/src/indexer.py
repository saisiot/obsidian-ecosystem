import json
import hashlib
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
from config import (
    VAULT_PATH,
    METADATA_FILE,
    EXCLUDE_PATTERNS,
    BACKUP_DIR,
    MAX_BACKUPS,
)
from obsidian_parser import ObsidianParser


class IncrementalIndexer:
    """ì¦ë¶„ ì¸ë±ì‹± ì‹œìŠ¤í…œ"""

    def __init__(self, vector_store):
        self.vault_path = VAULT_PATH
        self.vector_store = vector_store
        self.parser = ObsidianParser()
        self.metadata = self.load_metadata()
        self.network_store = None  # UnifiedIndexerì—ì„œ ì‚¬ìš©
        self.repomix_store = None  # UnifiedIndexerì—ì„œ ì‚¬ìš©

    def load_metadata(self) -> Dict:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if METADATA_FILE.exists():
            with open(METADATA_FILE, "r") as f:
                return json.load(f)
        return {"last_update": 0, "indexed_files": {}}

    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        METADATA_FILE.parent.mkdir(exist_ok=True)
        with open(METADATA_FILE, "w") as f:
            json.dump(self.metadata, f, indent=2)

    def get_md_files(self) -> Set[Path]:
        """Vault ì „ì²´ì˜ ëª¨ë“  .md íŒŒì¼ ìˆ˜ì§‘ (EXCLUDE_PATTERNS ì œì™¸)"""
        # vault ì „ì²´ì—ì„œ .md íŒŒì¼ ì°¾ê¸°
        all_md_files = self.vault_path.rglob("*.md")

        # ì œì™¸ íŒ¨í„´ í•„í„°ë§
        filtered_files = set()
        for file in all_md_files:
            skip = False
            # íŒŒì¼ì˜ ìƒëŒ€ ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            relative_path = str(file.relative_to(self.vault_path))

            # ê²½ë¡œì˜ ëª¨ë“  ë¶€ë¶„ í™•ì¸ (í´ë” ë° íŒŒì¼ëª…)
            path_parts = relative_path.split('/')

            for pattern in EXCLUDE_PATTERNS:
                # ".*" íŒ¨í„´: ì ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í´ë”/íŒŒì¼ ì œì™¸
                if pattern == ".*":
                    # ê²½ë¡œì˜ ì–´ëŠ ë¶€ë¶„ì´ë“  .ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œì™¸
                    if any(part.startswith('.') for part in path_parts):
                        skip = True
                        break
                else:
                    # ì¼ë°˜ íŒ¨í„´: ê²½ë¡œ ì–´ë””ë“  í¬í•¨ë˜ë©´ ì œì™¸
                    if pattern in relative_path or pattern in file.name:
                        skip = True
                        break

            if not skip:
                filtered_files.add(file)

        return filtered_files

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ìƒì„±"""
        stat = file_path.stat()
        return hashlib.md5(f"{stat.st_mtime}_{stat.st_size}".encode()).hexdigest()

    def check_updates(self) -> Dict[str, List[Path]]:
        """ë³€ê²½ì‚¬í•­ í™•ì¸"""
        current_files = self.get_md_files()
        indexed_files = set(Path(p) for p in self.metadata["indexed_files"].keys())

        # ìƒˆ íŒŒì¼
        new_files = current_files - indexed_files

        # ì‚­ì œëœ íŒŒì¼
        deleted_files = indexed_files - current_files

        # ìˆ˜ì •ëœ íŒŒì¼
        modified_files = []
        for file in current_files & indexed_files:
            current_hash = self.get_file_hash(file)
            if self.metadata["indexed_files"][str(file)] != current_hash:
                modified_files.append(file)

        return {
            "new": list(new_files),
            "modified": modified_files,
            "deleted": list(deleted_files),
        }

    def update_index(self):
        """ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰"""
        changes = self.check_updates()

        total_changes = sum(len(v) for v in changes.values())
        if total_changes == 0:
            print("âœ… ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
            return

        print(
            f"ğŸ“Š ë³€ê²½ì‚¬í•­ ê°ì§€: ìƒˆ íŒŒì¼ {len(changes['new'])}, "
            f"ìˆ˜ì • {len(changes['modified'])}, ì‚­ì œ {len(changes['deleted'])}"
        )

        # ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬
        for file in changes["deleted"]:
            self.vector_store.delete_document(str(file))
            del self.metadata["indexed_files"][str(file)]
            print(f"  âŒ ì‚­ì œ: {file.name}")

        # ìˆ˜ì •ëœ íŒŒì¼ ì²˜ë¦¬
        for file in changes["modified"]:
            doc = self.parser.parse_file(file)
            self.vector_store.update_document(doc)
            self.metadata["indexed_files"][str(file)] = self.get_file_hash(file)
            print(f"  â™»ï¸ ì—…ë°ì´íŠ¸: {file.name}")

        # ìƒˆ íŒŒì¼ ì²˜ë¦¬
        for file in changes["new"]:
            doc = self.parser.parse_file(file)
            self.vector_store.add_document(doc)
            self.metadata["indexed_files"][str(file)] = self.get_file_hash(file)
            print(f"  â• ì¶”ê°€: {file.name}")

        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        self.metadata["last_update"] = datetime.now().isoformat()
        self.save_metadata()

        print("âœ… ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")


class UnifiedIndexer(IncrementalIndexer):
    """í†µí•© ì¸ë±ì„œ: ChromaDB, NetworkMetadataStore, RepomixIndexStoreë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸"""

    def __init__(self, vector_store, network_store, repomix_store):
        """
        Args:
            vector_store: ChromaDB VectorStore ì¸ìŠ¤í„´ìŠ¤
            network_store: NetworkMetadataStore ì¸ìŠ¤í„´ìŠ¤
            repomix_store: RepomixIndexStore ì¸ìŠ¤í„´ìŠ¤
        """
        super().__init__(vector_store)
        self.network_store = network_store
        self.repomix_store = repomix_store

    def _create_backup(self) -> Path:
        """ì—…ë°ì´íŠ¸ ì „ ë°±ì—… ìŠ¤ëƒ…ìƒ· ìƒì„±

        Returns:
            backup_dir: ë°±ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = BACKUP_DIR / timestamp
        backup_dir.mkdir(parents=True, exist_ok=True)

        # 3ê°œ ë©”íƒ€ë°ì´í„° íŒŒì¼ ë°±ì—…
        if METADATA_FILE.exists():
            shutil.copy(METADATA_FILE, backup_dir / "index_metadata.json")

        if self.network_store.metadata_file.exists():
            shutil.copy(
                self.network_store.metadata_file, backup_dir / "network_metadata.json"
            )

        if self.repomix_store.index_file.exists():
            shutil.copy(
                self.repomix_store.index_file, backup_dir / "repomix_index.json"
            )

        print(f"ğŸ“¦ ë°±ì—… ìƒì„±: {backup_dir.name}")
        return backup_dir

    def _rollback(self, backup_dir: Path):
        """ë°±ì—…ì—ì„œ ë³µì›

        Args:
            backup_dir: ë³µì›í•  ë°±ì—… ë””ë ‰í† ë¦¬
        """
        try:
            print(f"ğŸ”„ ë¡¤ë°± ì‹œì‘: {backup_dir.name}")

            # ë©”íƒ€ë°ì´í„° íŒŒì¼ ë³µì›
            if (backup_dir / "index_metadata.json").exists():
                shutil.copy(backup_dir / "index_metadata.json", METADATA_FILE)

            if (backup_dir / "network_metadata.json").exists():
                shutil.copy(
                    backup_dir / "network_metadata.json",
                    self.network_store.metadata_file,
                )

            if (backup_dir / "repomix_index.json").exists():
                shutil.copy(
                    backup_dir / "repomix_index.json", self.repomix_store.index_file
                )

            # ë©”íƒ€ë°ì´í„° ì¬ë¡œë“œ
            self.metadata = self.load_metadata()
            self.network_store.metadata = self.network_store.load_metadata()
            self.repomix_store.index = self.repomix_store.load_index()

            print("âœ… ë¡¤ë°± ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ë¡¤ë°± ì‹¤íŒ¨: {e}")
            raise

    def _cleanup_old_backups(self, max_backups: int = MAX_BACKUPS):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬

        Args:
            max_backups: ìœ ì§€í•  ìµœëŒ€ ë°±ì—… ê°œìˆ˜
        """
        if not BACKUP_DIR.exists():
            return

        backups = sorted(BACKUP_DIR.iterdir(), reverse=True)

        # ìµœì‹  Nê°œë§Œ ìœ ì§€
        for old_backup in backups[max_backups:]:
            if old_backup.is_dir():
                shutil.rmtree(old_backup)
                print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {old_backup.name}")

    def update_index(self):
        """3ê°œ DB í†µí•© ì—…ë°ì´íŠ¸ (íŠ¸ëœì­ì…˜ ì§€ì›)"""
        changes = self.check_updates()

        if not changes["new"] and not changes["modified"] and not changes["deleted"]:
            print("âœ… ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
            return

        print(
            f"ğŸ“Š ë³€ê²½ì‚¬í•­ ê°ì§€: ìƒˆ íŒŒì¼ {len(changes['new'])}, "
            f"ìˆ˜ì • {len(changes['modified'])}, ì‚­ì œ {len(changes['deleted'])}"
        )

        # ë°±ì—… ìƒì„±
        backup_dir = self._create_backup()

        try:
            # Phase 1: Prepare updates for all 3 DBs
            updates = {"chroma": [], "network": [], "repomix": []}

            for file in changes["new"] + changes["modified"]:
                doc = self.parser.parse_file(file)

                # ChromaDB: full document
                updates["chroma"].append({"file": file, "doc": doc})

                # Network: extract links and tags
                updates["network"].append(
                    {
                        "path": str(file),
                        "title": doc["title"],
                        "para_folder": doc["para_folder"],
                        "forward_links": doc["wiki_links"],
                        "tags": doc["tags"],
                        "metadata": doc.get("metadata", {}),
                        "content": doc["content"],
                    }
                )

                # Repomix: extract file stats
                updates["repomix"].append(
                    {
                        "doc": doc,
                        "file": file,
                    }
                )

            # Phase 2: Apply updates to all 3 DBs
            # Update ChromaDB (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
            for file in changes["deleted"]:
                self.vector_store.delete_document(str(file))
                del self.metadata["indexed_files"][str(file)]
                print(f"  âŒ ì‚­ì œ: {file.name}")

            # ìˆ˜ì •ëœ íŒŒì¼ê³¼ ìƒˆ íŒŒì¼ ì²˜ë¦¬
            for i, file in enumerate(changes["modified"] + changes["new"]):
                chroma_data = updates["chroma"][i]
                doc = chroma_data["doc"]

                if file in changes["modified"]:
                    self.vector_store.update_document(doc)
                    print(f"  â™»ï¸ ì—…ë°ì´íŠ¸: {file.name}")
                else:
                    self.vector_store.add_document(doc)
                    print(f"  â• ì¶”ê°€: {file.name}")

                self.metadata["indexed_files"][str(file)] = self.get_file_hash(file)

            # Update NetworkMetadataStore
            print("  ğŸ”— ë„¤íŠ¸ì›Œí¬ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
            for network_doc in updates["network"]:
                self.network_store.update_metadata(network_doc)

            for file in changes["deleted"]:
                self.network_store.delete_metadata(str(file))

            # Update RepomixIndexStore
            print("  ğŸ“¦ Repomix ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
            for repomix_data in updates["repomix"]:
                doc = repomix_data["doc"]
                file = repomix_data["file"]
                self.repomix_store.update_index(doc, file)

            for file in changes["deleted"]:
                self.repomix_store.delete_index(str(file))

            # Save all metadata
            self.metadata["last_update"] = datetime.now().isoformat()
            self.save_metadata()
            self.network_store.save_metadata()
            self.repomix_store.save_index()

            print("âœ… í†µí•© ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            print(
                f"  - ChromaDB: {len(changes['new']) + len(changes['modified'])} ë¬¸ì„œ ì—…ë°ì´íŠ¸"
            )
            print(f"  - Network: {len(updates['network'])} ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸")
            print(f"  - Repomix: {len(updates['repomix'])} ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸")

            # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
            self._cleanup_old_backups()

        except Exception as e:
            print(f"âŒ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ë¡¤ë°± ì‹œë„ ì¤‘...")
            self._rollback(backup_dir)
            raise
